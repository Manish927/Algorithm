{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7xOtY1PJZgkEBtGHYe5l8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manish927/Algorithm/blob/patch1/SearchEngine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outlining how Google search works and a Python implementation.\n",
        "## Core components:\n",
        "Web crawling: discover and fetch pages\n",
        "Indexing: build an inverted index\n",
        "Ranking: score and order results\n",
        "Query processing: parse and match queries\n",
        "Python implementation:\n",
        "\n",
        "# How Google Search Works\n",
        "Core components:\n",
        "Web Crawling: Discovers and downloads web pages\n",
        "Indexing: Builds an inverted index mapping words to documents\n",
        "Ranking: Scores pages using algorithms like PageRank and TF-IDF\n",
        "Query Processing: Parses queries and retrieves relevant results\n",
        "A Python implementation:"
      ],
      "metadata": {
        "id": "Q5Yp41DkMqT8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_3IheW1L91B",
        "outputId": "243a56b0-448a-416b-e8c8-938a8f9a99ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting web crawl...\n",
            "Indexing 0 documents...\n",
            "Computing PageRank scores...\n",
            "Indexed 0 documents\n",
            "Index contains 0 unique words\n",
            "\n",
            "==================================================\n",
            "Search Results\n",
            "==================================================\n",
            "\n",
            "Query: 'python programming'\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: 'web development'\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: 'data structures'\n",
            "--------------------------------------------------\n",
            "\n",
            "Index saved to search_index.json\n"
          ]
        }
      ],
      "source": [
        "# search_engine.py\n",
        "import re\n",
        "import math\n",
        "from collections import defaultdict, Counter\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from urllib.robotparser import RobotFileParser\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import List, Dict, Set, Tuple\n",
        "import json\n",
        "import time\n",
        "\n",
        "class WebCrawler:\n",
        "    \"\"\"Crawls web pages and extracts content\"\"\"\n",
        "\n",
        "    def __init__(self, max_pages: int = 100, delay: float = 1.0):\n",
        "        self.max_pages = max_pages\n",
        "        self.delay = delay\n",
        "        self.visited = set()\n",
        "        self.robots_parser = {}\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (compatible; SearchBot/1.0)'\n",
        "        })\n",
        "\n",
        "    def can_fetch(self, url: str) -> bool:\n",
        "        \"\"\"Check if URL can be fetched according to robots.txt\"\"\"\n",
        "        parsed = urlparse(url)\n",
        "        base_url = f\"{parsed.scheme}://{parsed.netloc}\"\n",
        "\n",
        "        if base_url not in self.robots_parser:\n",
        "            robots_url = urljoin(base_url, '/robots.txt')\n",
        "            try:\n",
        "                rp = RobotFileParser()\n",
        "                rp.set_url(robots_url)\n",
        "                rp.read()\n",
        "                self.robots_parser[base_url] = rp\n",
        "            except:\n",
        "                self.robots_parser[base_url] = None\n",
        "\n",
        "        parser = self.robots_parser[base_url]\n",
        "        return parser is None or parser.can_fetch('*', url)\n",
        "\n",
        "    def extract_links(self, html: str, base_url: str) -> List[str]:\n",
        "        \"\"\"Extract all links from HTML\"\"\"\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        links = []\n",
        "\n",
        "        for tag in soup.find_all('a', href=True):\n",
        "            href = tag['href']\n",
        "            absolute_url = urljoin(base_url, href)\n",
        "            parsed = urlparse(absolute_url)\n",
        "\n",
        "            # Only follow http/https links\n",
        "            if parsed.scheme in ['http', 'https']:\n",
        "                links.append(absolute_url)\n",
        "\n",
        "        return links\n",
        "\n",
        "    def extract_text(self, html: str) -> str:\n",
        "        \"\"\"Extract text content from HTML\"\"\"\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script in soup([\"script\", \"style\"]):\n",
        "            script.decompose()\n",
        "\n",
        "        # Get text\n",
        "        text = soup.get_text()\n",
        "\n",
        "        # Clean up whitespace\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def crawl(self, start_url: str) -> List[Dict]:\n",
        "        \"\"\"Crawl web starting from start_url\"\"\"\n",
        "        queue = [start_url]\n",
        "        pages = []\n",
        "\n",
        "        while queue and len(pages) < self.max_pages:\n",
        "            url = queue.pop(0)\n",
        "\n",
        "            if url in self.visited:\n",
        "                continue\n",
        "\n",
        "            if not self.can_fetch(url):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                print(f\"Crawling: {url}\")\n",
        "                response = self.session.get(url, timeout=10)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                if 'text/html' not in response.headers.get('Content-Type', ''):\n",
        "                    continue\n",
        "\n",
        "                html = response.text\n",
        "                text = self.extract_text(html)\n",
        "\n",
        "                if len(text) < 100:  # Skip pages with too little content\n",
        "                    continue\n",
        "\n",
        "                pages.append({\n",
        "                    'url': url,\n",
        "                    'title': self.extract_title(html),\n",
        "                    'text': text,\n",
        "                    'links': self.extract_links(html, url)\n",
        "                })\n",
        "\n",
        "                self.visited.add(url)\n",
        "\n",
        "                # Add new links to queue\n",
        "                for link in self.extract_links(html, url):\n",
        "                    if link not in self.visited and len(queue) < 1000:\n",
        "                        queue.append(link)\n",
        "\n",
        "                time.sleep(self.delay)  # Be polite\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error crawling {url}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return pages\n",
        "\n",
        "    def extract_title(self, html: str) -> str:\n",
        "        \"\"\"Extract page title\"\"\"\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        title_tag = soup.find('title')\n",
        "        return title_tag.get_text() if title_tag else \"Untitled\"\n",
        "\n",
        "\n",
        "class Indexer:\n",
        "    \"\"\"Builds inverted index from documents\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.inverted_index = defaultdict(dict)  # word -> {doc_id: tf}\n",
        "        self.documents = {}  # doc_id -> document metadata\n",
        "        self.doc_id_counter = 0\n",
        "        self.idf_cache = {}\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Tokenize text into words\"\"\"\n",
        "        # Convert to lowercase and split on non-word characters\n",
        "        words = re.findall(r'\\b[a-z]+\\b', text.lower())\n",
        "        return words\n",
        "\n",
        "    def compute_tf(self, words: List[str]) -> Dict[str, float]:\n",
        "        \"\"\"Compute Term Frequency\"\"\"\n",
        "        word_count = Counter(words)\n",
        "        total_words = len(words)\n",
        "\n",
        "        # Normalized TF\n",
        "        tf = {word: count / total_words for word, count in word_count.items()}\n",
        "        return tf\n",
        "\n",
        "    def add_document(self, doc: Dict):\n",
        "        \"\"\"Add document to index\"\"\"\n",
        "        doc_id = self.doc_id_counter\n",
        "        self.doc_id_counter += 1\n",
        "\n",
        "        text = doc.get('text', '')\n",
        "        words = self.tokenize(text)\n",
        "\n",
        "        if not words:\n",
        "            return\n",
        "\n",
        "        # Compute TF for this document\n",
        "        tf = self.compute_tf(words)\n",
        "\n",
        "        # Add to inverted index\n",
        "        for word, tf_value in tf.items():\n",
        "            self.inverted_index[word][doc_id] = tf_value\n",
        "\n",
        "        # Store document metadata\n",
        "        self.documents[doc_id] = {\n",
        "            'url': doc['url'],\n",
        "            'title': doc.get('title', 'Untitled'),\n",
        "            'text': text[:500] + '...' if len(text) > 500 else text,  # Store snippet\n",
        "            'word_count': len(words)\n",
        "        }\n",
        "\n",
        "    def compute_idf(self, word: str) -> float:\n",
        "        \"\"\"Compute Inverse Document Frequency\"\"\"\n",
        "        if word in self.idf_cache:\n",
        "            return self.idf_cache[word]\n",
        "\n",
        "        doc_frequency = len(self.inverted_index[word])\n",
        "        total_docs = len(self.documents)\n",
        "\n",
        "        if doc_frequency == 0:\n",
        "            idf = 0\n",
        "        else:\n",
        "            idf = math.log(total_docs / doc_frequency)\n",
        "\n",
        "        self.idf_cache[word] = idf\n",
        "        return idf\n",
        "\n",
        "    def compute_tfidf(self, word: str, doc_id: int) -> float:\n",
        "        \"\"\"Compute TF-IDF score\"\"\"\n",
        "        if doc_id not in self.inverted_index[word]:\n",
        "            return 0\n",
        "\n",
        "        tf = self.inverted_index[word][doc_id]\n",
        "        idf = self.compute_idf(word)\n",
        "\n",
        "        return tf * idf\n",
        "\n",
        "\n",
        "class PageRank:\n",
        "    \"\"\"PageRank algorithm for ranking pages\"\"\"\n",
        "\n",
        "    def __init__(self, damping_factor: float = 0.85, iterations: int = 10):\n",
        "        self.damping_factor = damping_factor\n",
        "        self.iterations = iterations\n",
        "\n",
        "    def compute(self, documents: List[Dict]) -> Dict[str, float]:\n",
        "        \"\"\"Compute PageRank scores\"\"\"\n",
        "        # Build link graph\n",
        "        url_to_id = {doc['url']: i for i, doc in enumerate(documents)}\n",
        "        id_to_url = {i: doc['url'] for i, doc in enumerate(documents)}\n",
        "\n",
        "        # Build adjacency list\n",
        "        graph = defaultdict(list)\n",
        "        for i, doc in enumerate(documents):\n",
        "            for link in doc.get('links', []):\n",
        "                if link in url_to_id:\n",
        "                    graph[i].append(url_to_id[link])\n",
        "\n",
        "        # Initialize PageRank scores\n",
        "        n = len(documents)\n",
        "        pr = {i: 1.0 / n for i in range(n)}\n",
        "\n",
        "        # Iterate PageRank algorithm\n",
        "        for _ in range(self.iterations):\n",
        "            new_pr = {}\n",
        "            for i in range(n):\n",
        "                # Damping factor contribution\n",
        "                new_pr[i] = (1 - self.damping_factor) / n\n",
        "\n",
        "                # Sum contributions from incoming links\n",
        "                for j in range(n):\n",
        "                    if i in graph[j]:\n",
        "                        out_links = len(graph[j])\n",
        "                        if out_links > 0:\n",
        "                            new_pr[i] += self.damping_factor * pr[j] / out_links\n",
        "\n",
        "            pr = new_pr\n",
        "\n",
        "        # Convert to URL-based scores\n",
        "        return {id_to_url[i]: score for i, score in pr.items()}\n",
        "\n",
        "\n",
        "class SearchEngine:\n",
        "    \"\"\"Main search engine class\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.indexer = Indexer()\n",
        "        self.page_rank = PageRank()\n",
        "        self.page_rank_scores = {}\n",
        "\n",
        "    def index_documents(self, documents: List[Dict]):\n",
        "        \"\"\"Index a collection of documents\"\"\"\n",
        "        print(f\"Indexing {len(documents)} documents...\")\n",
        "\n",
        "        for doc in documents:\n",
        "            self.indexer.add_document(doc)\n",
        "\n",
        "        # Compute PageRank scores\n",
        "        print(\"Computing PageRank scores...\")\n",
        "        self.page_rank_scores = self.page_rank.compute(documents)\n",
        "\n",
        "        print(f\"Indexed {len(self.indexer.documents)} documents\")\n",
        "        print(f\"Index contains {len(self.indexer.inverted_index)} unique words\")\n",
        "\n",
        "    def search(self, query: str, top_k: int = 10) -> List[Dict]:\n",
        "        \"\"\"Search for documents matching query\"\"\"\n",
        "        query_words = self.indexer.tokenize(query)\n",
        "\n",
        "        if not query_words:\n",
        "            return []\n",
        "\n",
        "        # Score each document\n",
        "        doc_scores = defaultdict(float)\n",
        "\n",
        "        for word in query_words:\n",
        "            if word not in self.indexer.inverted_index:\n",
        "                continue\n",
        "\n",
        "            idf = self.indexer.compute_idf(word)\n",
        "\n",
        "            for doc_id in self.indexer.inverted_index[word]:\n",
        "                tfidf = self.indexer.compute_tfidf(word, doc_id)\n",
        "                doc_scores[doc_id] += tfidf\n",
        "\n",
        "        # Combine TF-IDF with PageRank\n",
        "        results = []\n",
        "        for doc_id, tfidf_score in doc_scores.items():\n",
        "            doc = self.indexer.documents[doc_id]\n",
        "            url = doc['url']\n",
        "\n",
        "            # Combine scores (weighted combination)\n",
        "            pagerank_score = self.page_rank_scores.get(url, 0.0)\n",
        "            combined_score = 0.7 * tfidf_score + 0.3 * pagerank_score\n",
        "\n",
        "            results.append({\n",
        "                'doc_id': doc_id,\n",
        "                'url': url,\n",
        "                'title': doc['title'],\n",
        "                'snippet': doc['text'],\n",
        "                'score': combined_score,\n",
        "                'tfidf_score': tfidf_score,\n",
        "                'pagerank_score': pagerank_score\n",
        "            })\n",
        "\n",
        "        # Sort by score and return top K\n",
        "        results.sort(key=lambda x: x['score'], reverse=True)\n",
        "        return results[:top_k]\n",
        "\n",
        "    def save_index(self, filepath: str):\n",
        "        \"\"\"Save index to file\"\"\"\n",
        "        data = {\n",
        "            'inverted_index': {k: dict(v) for k, v in self.indexer.inverted_index.items()},\n",
        "            'documents': self.indexer.documents,\n",
        "            'page_rank_scores': self.page_rank_scores\n",
        "        }\n",
        "\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(data, f, indent=2)\n",
        "\n",
        "    def load_index(self, filepath: str):\n",
        "        \"\"\"Load index from file\"\"\"\n",
        "        with open(filepath, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        self.indexer.inverted_index = defaultdict(dict, {\n",
        "            k: defaultdict(float, v) for k, v in data['inverted_index'].items()\n",
        "        })\n",
        "        self.indexer.documents = data['documents']\n",
        "        self.page_rank_scores = data['page_rank_scores']\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize components\n",
        "    crawler = WebCrawler(max_pages=50, delay=1.0)\n",
        "    search_engine = SearchEngine()\n",
        "\n",
        "    # Crawl web pages (example: Wikipedia)\n",
        "    print(\"Starting web crawl...\")\n",
        "    documents = crawler.crawl(\"https://en.wikipedia.org/wiki/Python_(programming_language)\")\n",
        "\n",
        "    # Index documents\n",
        "    search_engine.index_documents(documents)\n",
        "\n",
        "    # Perform searches\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Search Results\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    queries = [\"python programming\", \"web development\", \"data structures\"]\n",
        "\n",
        "    for query in queries:\n",
        "        print(f\"\\nQuery: '{query}'\")\n",
        "        print(\"-\" * 50)\n",
        "        results = search_engine.search(query, top_k=5)\n",
        "\n",
        "        for i, result in enumerate(results, 1):\n",
        "            print(f\"\\n{i}. {result['title']}\")\n",
        "            print(f\"   URL: {result['url']}\")\n",
        "            print(f\"   Score: {result['score']:.4f}\")\n",
        "            print(f\"   Snippet: {result['snippet'][:200]}...\")\n",
        "\n",
        "    # Save index for later use\n",
        "    search_engine.save_index(\"search_index.json\")\n",
        "    print(\"\\nIndex saved to search_index.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key features:\n",
        "WebCrawler: Respects robots.txt, extracts links and text\n",
        "Indexer: Builds inverted index with TF-IDF\n",
        "PageRank: Computes page importance from link graph\n",
        "SearchEngine: Combines TF-IDF and PageRank for ranking\n",
        "# How to use:"
      ],
      "metadata": {
        "id": "mm_NytUPOStd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# requirements.txt\n",
        "requests>=2.31.0\n",
        "beautifulsoup4>=4.12.0\n",
        "lxml>=4.9.0"
      ],
      "metadata": {
        "id": "jG53zpabPhGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "pip install requests beautifulsoup4 lxml\n",
        "\n",
        "# Run the search engine\n",
        "python search_engine.py"
      ],
      "metadata": {
        "id": "hn4FGmJzOkxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplified version for local documents\n",
        "\n",
        "# simple_search.py\n",
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict\n",
        "\n",
        "class SimpleSearchEngine:\n",
        "    \"\"\"Simple search engine for local text files\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.index = defaultdict(set)  # word -> set of file paths\n",
        "        self.documents = {}  # file_path -> content\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract words from text\"\"\"\n",
        "        return re.findall(r'\\b[a-z]+\\b', text.lower())\n",
        "\n",
        "    def index_file(self, filepath: str):\n",
        "        \"\"\"Index a text file\"\"\"\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            self.documents[filepath] = content\n",
        "            words = self.tokenize(content)\n",
        "\n",
        "            for word in words:\n",
        "                self.index[word].add(filepath)\n",
        "\n",
        "            print(f\"Indexed: {filepath} ({len(words)} words)\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error indexing {filepath}: {e}\")\n",
        "\n",
        "    def index_directory(self, directory: str):\n",
        "        \"\"\"Index all .txt files in directory\"\"\"\n",
        "        for root, dirs, files in os.walk(directory):\n",
        "            for file in files:\n",
        "                if file.endswith('.txt'):\n",
        "                    filepath = os.path.join(root, file)\n",
        "                    self.index_file(filepath)\n",
        "\n",
        "    def search(self, query: str) -> List[str]:\n",
        "        \"\"\"Search for files containing query words\"\"\"\n",
        "        query_words = self.tokenize(query)\n",
        "\n",
        "        if not query_words:\n",
        "            return []\n",
        "\n",
        "        # Find files containing all query words (AND search)\n",
        "        result_files = None\n",
        "        for word in query_words:\n",
        "            if word in self.index:\n",
        "                files = self.index[word]\n",
        "                if result_files is None:\n",
        "                    result_files = files.copy()\n",
        "                else:\n",
        "                    result_files &= files  # Intersection\n",
        "\n",
        "        return list(result_files) if result_files else []\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    engine = SimpleSearchEngine()\n",
        "\n",
        "    # Index a directory\n",
        "    engine.index_directory(\"./documents\")\n",
        "\n",
        "    # Search\n",
        "    results = engine.search(\"python programming\")\n",
        "    print(f\"Found {len(results)} files:\")\n",
        "    for file in results:\n",
        "        print(f\"  - {file}\")"
      ],
      "metadata": {
        "id": "bhQgewnnOqe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It covers the core concepts. For production, add:\n",
        "- Distributed crawling\n",
        "- Database storage\n",
        "- Caching\n",
        "- Query expansion\n",
        "- Spell correction\n",
        "- Advanced ranking signals\n",
        "Should I add any specific features or explain any part in more detail?"
      ],
      "metadata": {
        "id": "Go_hmslyO8Up"
      }
    }
  ]
}